{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a plan for implementing a Digit Recognition project using TensorFlow Lite on Ubuntu:\n",
    "1. Collect and prepare the dataset: Collect a dataset of images of handwritten digits, such as the MNIST dataset. Preprocess the images to resize and normalize them.\n",
    "2. Train a model: Train a convolutional neural network (CNN) model on the dataset using TensorFlow or another deep learning framework.\n",
    "3. Convert the model to TensorFlow Lite format: Convert the trained model to TensorFlow Lite format using the tflite_convert command-line tool.\n",
    "4. Load and run inference on TensorFlow Lite model: Use the TensorFlow Lite Python API to load the TensorFlow Lite model and run inference on new images.\n",
    "5. Post-processing: Post-process the output of the model to get the final predicted digit and its confidence.\n",
    "6. Create a user interface: Create a user interface using React.js or another framework that allows users to upload an image and displays the predicted digit and its confidence.\n",
    "7. Integrate the model into the web application: Integrate the TensorFlow Lite model into the web application using the .NET web API.\n",
    "8. Evaluate the model performance: Evaluate the model performance on a held-out test set and make adjustments as needed.\n",
    "9. Deploy the application: Deploy the web application to a server or hosting platform so that users can access it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of how you can collect and prepare the dataset for a Digit Recognition project using the MNIST dataset in Python:\n",
    "pip install tensorflow \n",
    "pip install keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 03:57:59.278848: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from PIL import Image\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the MNIST dataset:\n",
    "Modified National Institute of Standards and Technology database[1]) is a large database of handwritten digits that is commonly used for training various image processing systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data: You can preprocess the data by normalizing the pixel values and converting them to float32:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the data: You can reshape the data to fit the input shape of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Define number of classes\n",
    "num_classes = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert labels to categorical: You can convert the labels to categorical format for use in training the model with a categorical loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data: You can save the data to a file so that you can use it later without loading the data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_train.npy', x_train)\n",
    "np.save('y_train.npy', y_train)\n",
    "np.save('x_test.npy', x_test)\n",
    "np.save('y_test.npy', y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed data:\n",
    "x_train = np.load('x_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "x_test = np.load('x_test.npy')\n",
    "y_test = np.load('y_test.npy')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model: Define the architecture of the model using the Keras functional API or Sequential API. Here's an example of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-16 03:58:05.030922: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-16 03:58:05.035035: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(128, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model: Before training, the model has to be compiled with a loss function, an optimizer and metrics. Here is an example of compiling the model using the categorical crossentropy loss function, the Adam optimizer and accuracy as a metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model: You can train the model on the preprocessed data using the fit() method. Here's an example of training the model for 5 epochs with a batch size of 128:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 22s 45ms/step - loss: 0.2010 - accuracy: 0.9419\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 20s 43ms/step - loss: 0.0579 - accuracy: 0.9829\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 19s 40ms/step - loss: 0.0392 - accuracy: 0.9881\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 0.0304 - accuracy: 0.9905\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 17s 36ms/step - loss: 0.0239 - accuracy: 0.9925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f363d44b7f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5, batch_size=128)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model: You can evaluate the performance of the model on the test set using the evaluate() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.0307 - accuracy: 0.9894\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model: You can save the trained model so that you can use it later without training it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mnist_cnn.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert the model to TensorFlow Lite format\n",
    "1. Install the TensorFlow Lite Converter:\n",
    "pip install tensorflow==2.4.0\n",
    "\n",
    "2. Convert the model to TensorFlow Lite format:\n",
    "tflite_convert --output_file=model.tflite --keras_model_file=mnist_cnn.h5\n",
    "\n",
    "In this example, mnist_cnn.h5 is the file containing the trained model and model.tflite is the output file containing the TensorFlow Lite model.\n",
    "\n",
    "You can also convert the model using the TensorFlow Lite Converter Python API:\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model_file('mnist_cnn.h5')\n",
    "tflite_model = converter.convert()\n",
    "open(\"model.tflite\", \"wb\").write(tflite_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpnt61n4wb/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpnt61n4wb/assets\n",
      "2023-01-16 03:59:44.511668: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-01-16 03:59:44.511708: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-01-16 03:59:44.512461: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpnt61n4wb\n",
      "2023-01-16 03:59:44.514048: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-01-16 03:59:44.514078: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpnt61n4wb\n",
      "2023-01-16 03:59:44.515027: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2023-01-16 03:59:44.519502: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2023-01-16 03:59:44.520704: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-01-16 03:59:44.579727: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmpnt61n4wb\n",
      "2023-01-16 03:59:44.590150: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 77691 microseconds.\n",
      "2023-01-16 03:59:44.621810: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-01-16 03:59:44.649797: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "903536"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_model = keras.models.load_model('mnist_cnn.h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "tflite_model = converter.convert()\n",
    "open(\"model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load and run inference on TensorFlow Lite model: Use the TensorFlow Lite Python API to load the TensorFlow Lite model and run inference on new images.\n",
    "\n",
    "#### 1. Load the TensorFlow Lite model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path='model.tflite')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Allocate memory for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Get input and output tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor_index = interpreter.get_input_details()[0]['index']\n",
    "output_tensor_index = interpreter.get_output_details()[0]['index']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Provide input data and run inference:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Get Input Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 446988 into shape (1,28,28,1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m img_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(img)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Reshape the array to match the expected input shape of the model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# For example, if the model expects (1, 28, 28, 1) for the MNIST dataset\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m img_data \u001b[39m=\u001b[39m img_data\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m28\u001b[39;49m, \u001b[39m28\u001b[39;49m, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m \u001b[39m# Normalize the pixel values if necessary\u001b[39;00m\n\u001b[1;32m      9\u001b[0m img_data \u001b[39m=\u001b[39m img_data \u001b[39m/\u001b[39m \u001b[39m255\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 446988 into shape (1,28,28,1)"
     ]
    }
   ],
   "source": [
    "img = Image.open('single-digit-sample.jpg')\n",
    "# Convert the image to a numpy array\n",
    "img_data = np.array(img)\n",
    "# Reshape the array to match the expected input shape of the model\n",
    "# For example, if the model expects (1, 28, 28, 1) for the MNIST dataset\n",
    "img_data = img_data.reshape(1, 28, 28, 1)\n",
    "\n",
    "# Normalize the pixel values if necessary\n",
    "img_data = img_data / 255\n",
    "input_data = img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.set_tensor(input_tensor_index, input_data)\n",
    "interpreter.invoke()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Fetch the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = interpreter.get_tensor(output_tensor_index)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Post-processing: Post-process the output of the model to get the final predicted digit and its confidence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of how you can post-process the output of a TensorFlow Lite model to get the final predicted digit and its confidence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted digit:  8\n",
      "Confidence:  4.4632683e+30\n"
     ]
    }
   ],
   "source": [
    "# Get the output data from the interpreter\n",
    "output_data = interpreter.get_tensor(output_tensor_index)\n",
    "\n",
    "# Get the index of the highest probability\n",
    "predicted_digit = np.argmax(output_data)\n",
    "\n",
    "# Get the corresponding probability\n",
    "confidence = output_data[0][predicted_digit]\n",
    "\n",
    "# Print the results\n",
    "print(\"Predicted digit: \", predicted_digit)\n",
    "print(\"Confidence: \", confidence)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use the np.argmax() function to get the index of the highest probability in the output data, which corresponds to the predicted digit.\n",
    "Then, we use the index to get the corresponding probability, which is the confidence of the prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    If you are building a simple app that only needs to recognize a single digit, you could use the code that I provided in my previous answer to get the predicted digit and its confidence, then display the results to the user.\n",
    "\n",
    "    If you are building an application that needs to recognize multiple digits in an image, you could use object detection techniques such as YOLO or Faster R-CNN to detect the bounding boxes of the digits in the image, then crop the digits and run the TensorFlow Lite model on each of the cropped images.\n",
    "    \n",
    "    If you are building an application that needs to recognize multiple digits in real-time, such as a point-of-sale system, you could use a library such as OpenCV to capture video frames, then use object detection techniques to detect the digits in the frames, and finally run the TensorFlow Lite model on each of the detected digits."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b155b3abb7849b8b59f4c3da27fa4dfe0b9ebf8b222d7c92932ca66867e3f466"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
