{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of how you can collect and label a dataset of images for an object detection project using the COCO (Common Objects in Context) dataset:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download the COCO dataset from the official website: http://cocodataset.org/#download\n",
    "2. Extract the dataset to a directory on your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "tar -xvf coco.tar.gz\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The COCO dataset contains a large number of images and annotations, so it's a good idea to create a subset of the dataset that contains only the classes of objects that you're interested in detecting. To do this, you can use the cocoapi library to filter the images and annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Load the annotations file\n",
    "with open('path/to/annotations/instances_val2017.json', 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Define the classes of interest\n",
    "class_ids = [1, 3]  # 1: person, 3: car\n",
    "\n",
    "# Create a list to store the filtered images\n",
    "filtered_images = []\n",
    "\n",
    "# Iterate through all images in the dataset\n",
    "for image in annotations['images']:\n",
    "    # Iterate through all annotations for the image\n",
    "    for annotation in annotations['annotations']:\n",
    "        if annotation['image_id'] == image['id'] and annotation['category_id'] in class_ids:\n",
    "            filtered_images.append(image)\n",
    "            break\n",
    "\n",
    "# Copy the filtered images to a new directory\n",
    "for image in filtered_images:\n",
    "    src_path = 'path/to/original/images/' + image['file_name']\n",
    "    dst_path = 'path/to/filtered/images/' + image['file_name']\n",
    "    shutil.copy(src_path, dst_path)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have downloaded and prepared the COCO dataset, you can use it to train an object detection model using TensorFlow's Object Detection API. Here's an example of how you can use the Object Detection API to train a model on the COCO dataset:\n",
    "\n",
    "1. Install the TensorFlow Object Detection API by following the instructions on the GitHub page: https://github.com/tensorflow/models/blob/main/research/object_detection/g3doc/tf2.md\n",
    "\n",
    "2. Convert the dataset to the TFRecord format, which is the input format for the TensorFlow Object Detection API. You can use the create_coco_tf_record.py script provided by the Object Detection API to convert the COCO dataset to the TFRecord format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python create_coco_tf_record.py --logtostderr \\\n",
    "  --train_image_dir=\"path/to/train/images\" \\\n",
    "  --val_image_dir=\"path/to/val/images\" \\\n",
    "  --test_image_dir=\"path/to/test/images\" \\\n",
    "  --train_annotations_file=\"path/to/annotations/train.json\" \\\n",
    "  --val_annotations_file=\"path/to/annotations/val.json\" \\\n",
    "  --output_dir=\"path/to/output/tfrecord\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Configure the training pipeline by creating a configuration file that specifies the model architecture, the dataset, and the training parameters. You can use one of the provided config files as a starting point and modify it to suit your needs.\n",
    "\n",
    "2. Start the training process using the TensorFlow Object Detection API's model_main.py script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python model_main.py --logtostderr \\\n",
    "  --model_dir=path/to/model/dir \\\n",
    "  --pipeline_config_path=path/to/config/file\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. After the training process is complete, you can export the trained model in a format that can be used in other applications, such as TensorFlow Lite for mobile or TensorRT for inference on NVIDIA GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python export_inference_graph.py --input_type image_tensor \\\n",
    "    --pipeline_config_path path/to/config/file \\\n",
    "    --trained_checkpoint_prefix path/to/trained/model \\\n",
    "    --output_directory path/to/exported/model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a python script that loads the trained model and use it to detect objects in images or video frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load the frozen Tensorflow model into memory\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile('path/to/exported/model', 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "        \n",
    "# Loading label map\n",
    "category_index = label_map_util.create_category_index_from_labelmap('path/to/label_map.pbtxt', use_display_name=True)\n",
    "\n",
    "# Helper code\n",
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape(\n",
    "        (im_height, im_width, 3)).astype(np.uint8)\n",
    "    \n",
    "# Detection\n",
    "with detection_graph.as_default():\n",
    "    with tf.Session(graph=detection_graph) as sess:\n",
    "        # Definite input and output Tensors for detection_graph\n",
    "        image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "        # Each box represents a part of the image where a particular object was detected\n",
    "        detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "        # Each score represent how level of confidence for each of the objects.\n",
    "        # Score is shown on the result image, together with the class label.\n",
    "        detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "        detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "        num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "        \n",
    "            # Detection for a single image\n",
    "        image = Image.open('path/to/image')\n",
    "        # the array based representation of the image will be used later in order to prepare the result image with boxes and labels on it.\n",
    "        image_np = load_image_into_numpy_array(image)\n",
    "        # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "        image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "        # Actual detection.\n",
    "        (boxes, scores, classes, num) = sess.run(\n",
    "            [detection_boxes, detection_scores, detection_classes, num_detections],\n",
    "            feed_dict={image_tensor: image_np_expanded})\n",
    "        # Visualization of the results of a detection.\n",
    "        vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "            image_np,\n",
    "            np.squeeze(boxes),\n",
    "            np.squeeze(classes).astype(np.int32),\n",
    "            np.squeeze(scores),\n",
    "            category_index,\n",
    "            use_normalized_coordinates=True,\n",
    "            line_thickness=8)\n",
    "        plt.figure(figsize=IMAGE_SIZE)\n",
    "        plt.imshow(image_np)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The first block of code loads the frozen TensorFlow model into memory by creating a detection_graph and importing the model graph and weights.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The label_map_util is used to create a category index from the label map file. The label map file is a text file that maps class IDs to class names, and is used to map the predicted class IDs to class names when visualizing the detection results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The load_image_into_numpy_array function is a helper function that converts an image file to a numpy array."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The with block that starts with with detection_graph.as_default(): is used to set the detection_graph as the default graph, so that all tensors and operations created within the block are added to the detection_graph."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. The with tf.Session(graph=detection_graph) as sess: block creates a TensorFlow session and sets the detection_graph as the default graph for the session."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. image_tensor = detection_graph.get_tensor_by_name('image_tensor:0') is used to get the tensor that represents the input image in the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. detection_boxes, detection_scores, detection_classes, and num_detections are used to get the tensors that represent the bounding boxes, scores, class IDs, and number of detections respectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. image_np_expanded = np.expand_dims(image_np, axis=0) is used to add an extra dimension to the image array, as the model expects a batch of images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. sess.run() runs the model on the image, and returns the bounding boxes, scores, class IDs, and number of detections."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. vis_util.visualize_boxes_and_labels_on_image_array() is used to draw the bounding boxes and class labels on the image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. plt.imshow(image_np)` is used to display the image with the detection results.\n",
    "\n",
    "It's also worth noting that this code is for a single image detection, you can also create a loop over all the images in a folder for batch detection and also you can use this in a video detection by using OpenCV and running the detection on each frame of the video."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have run the object detection on your images, here are some next steps you can take:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Evaluate the model performance: You can use the COCO evaluation metrics (such as mAP and AR) to evaluate the performance of your model. The TensorFlow Object Detection API provides a eval.py script that you can use to evaluate your model on the COCO dataset. You can run the script like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python eval.py --logtostderr \\\n",
    "    --pipeline_config_path=path/to/config/file \\\n",
    "    --checkpoint_dir=path/to/model/dir \\\n",
    "    --eval_dir=path/to/eval/dir\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Fine-tune the model: If the performance of your model is not satisfactory, you can fine-tune it on a smaller dataset that is more specific to your use case.\n",
    "2. Deploy the model: Once you are satisfied with the performance of your model, you can export it in a format that can be used in other applications. You can use TensorFlow Lite for mobile deployment, TensorRT for deployment on NVIDIA GPUs, or use the TensorFlow Serving to serve the model on a server.\n",
    "3. Use the model in real-time application: You can use the model in real-time applications by integrating it into a web or mobile application using TensorFlow.js or TensorFlow Lite."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploying a TensorFlow object detection model on Android involves several steps:\n",
    "1. Convert the TensorFlow model to TensorFlow Lite format: You can use the tflite_convert command-line tool to convert the TensorFlow model to TensorFlow Lite format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "tflite_convert \\\n",
    "    --output_file=path/to/output/model.tflite \\\n",
    "    --graph_def_file=path/to/frozen_model.pb \\\n",
    "    --input_arrays=input_tensor \\\n",
    "    --output_arrays=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3 \\\n",
    "    --input_shapes=1,300,300,3 \\\n",
    "    --inference_type=QUANTIZED_UINT8 \\\n",
    "    --mean_values=128 \\\n",
    "    --std_dev_values=128 \\\n",
    "    --change_concat_input_ranges=false \\\n",
    "    --allow_custom_ops\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Integrate TensorFlow Lite into your Android app: You can use the TensorFlow Lite Support Library to integrate TensorFlow Lite into your Android app. The library provides a Interpreter class that you can use to run inference on the TensorFlow Lite model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create an Android app that uses the TensorFlow Lite model: You can use the following code snippet to load the TensorFlow Lite model and run inference on an image in your Android app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "import org.tensorflow.lite.Interpreter;\n",
    "\n",
    "Interpreter tflite = new Interpreter(loadModelFile(context));\n",
    "\n",
    "private MappedByteBuffer loadModelFile(Context context) throws IOException {\n",
    "    AssetFileDescriptor fileDescriptor = context.getAssets().openFd(\"path/to/model.tflite\");\n",
    "    FileInputStream inputStream = new FileInputStream(fileDescriptor.getFileDescriptor());\n",
    "    FileChannel fileChannel = inputStream.getChannel();\n",
    "    long startOffset = fileDescriptor.getStartOffset();\n",
    "    long declaredLength = fileDescriptor.getDeclaredLength();\n",
    "    return fileChannel.map(FileChannel.MapMode.READ_ONLY, startOffset, declaredLength);\n",
    "}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run the inference: You can use the tflite.run() method to run inference on the TensorFlow Lite model with your input data.\n",
    "2. Post-processing of the output: You will get raw output of the model, you need to post-process it to get the final detection results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorRT for deployment on NVIDIA GPUs,\n",
    "1.     Install NVIDIA CUDA and cuDNN libraries: You will need to install the NVIDIA CUDA Toolkit and the cuDNN library on your Linux machine. CUDA is a parallel computing platform and programming model developed by NVIDIA, and cuDNN is a GPU-accelerated library for deep neural networks.\n",
    "\n",
    "2. Convert the TensorFlow model to TensorRT format: You can use the trtexec command-line tool to convert the TensorFlow model to TensorRT format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "trtexec --onnx=path/to/model.onnx --saveEngine=path/to/engine.plan\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Run the inference on TensorRT Engine: You can use the TensorRT C++ API to run inference on the TensorRT engine with your input data.\n",
    "2. Post-processing of the output: You will get raw output of the model, you need to post-process it to get the final detection results.\n",
    "3. Integrate the TensorRT engine into your application: You can use the TensorRT C++ API to integrate the TensorRT engine into your application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integrating a TensorRT object detection engine into a .NET & React.js web app involves several steps:\n",
    "1. Create a .NET Web API: You can create a .NET Web API that will handle the communication between the React.js front-end and the TensorRT engine.\n",
    "\n",
    "2. Integrate TensorRT engine into the Web API: You can use the TensorRT C++ API to integrate the TensorRT engine into the Web API. You can create a C++/CLI wrapper around the TensorRT C++ API and use it in your .NET Web API to run inference on the TensorRT engine.\n",
    "\n",
    "3. Create a React.js front-end: You can create a React.js front-end that allows the user to upload an image and displays the detection results.\n",
    "\n",
    "4. Connect the front-end and the back-end: You can use the Fetch API or a library like Axios to connect the React.js front-end to the .NET Web API.\n",
    "\n",
    "5. Post-processing of the output: You will get raw output of the model, you need to post-process it to get the final detection results and display them to the user."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
