{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    " (Adaptive Moment Estimation) is an optimization algorithm that can be used to update the parameters of a machine learning model. It is a popular choice for training deep learning models because it combines the benefits of two other optimization algorithms:\n",
    "\n",
    "- radient Descent: Adam uses the gradient of the loss function with respect to the parameters (weights and biases) of the model to update the parameters in the direction that minimizes the loss.\n",
    "\n",
    "- Momentum: Adam also uses a moving average of the gradient to smooth out the updates, which can help the optimization converge faster and be less sensitive to the choice of the learning rate.\n",
    "\n",
    "Adam is created by combining these ideas, it computes adaptive learning rates for each parameter. The algorithm calculates the exponentially weighted moving averages of the gradient and the squared gradient and the parameters are updated based on the ratio of the two.\n",
    "\n",
    "Adam is an algorithm that is generally robust to the choice of its hyperparameters, it is less memory-intensive than other optimization algorithms and it usually converge faster .\n",
    "\n",
    "Here's an example of how you can use the Adam optimizer in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
